{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlitzKriegM/C-digos-Proyecto/blob/main/llama3_rac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "\n",
        "# Debe instalarse por separado ya que Colab tiene torch 2.2.1, que rompe paquetes\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "if major_version >= 8:\n",
        "    # Utilice esto para nuevas GPU como Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Utilice esto para GPU m√°s antiguas (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "\n",
        "!pip install --upgrade trl\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments, AutoModelForCausalLM, TrainerCallback, TrainerState, TrainerControl\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "6_9We3lywgjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoggingCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, **kwargs):\n",
        "        logs = state.log_history[-1]\n",
        "        print(f\"Step {state.global_step}: {logs}\")\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16  # Ajustar dtype seg√∫n disponibilidad\n",
        "load_in_4bit = True\n",
        "\n",
        "# Cargar modelo y tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Configuraciones adicionales del modelo usando PEFT\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None\n",
        ")\n",
        "\n",
        "# Cargar y preparar el dataset\n",
        "dataset = load_dataset(\"somosnlp/Reglamento_Aeronautico_Colombiano_2024GemmaQA\", split=\"train\")\n",
        "dataset = dataset.shuffle(seed=1234)\n",
        "dataset = dataset.map(lambda samples: tokenizer(samples[\"Text\"]), batched=True)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyVMXLwdwhGI",
        "outputId": "05ec0a3e-1b4d-4124-9739-190925615e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TrainerCallback' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-64b53b56027d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLoggingCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainerCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {state.global_step}: {logs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TrainerCallback' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n de los par√°metros de entrenamiento\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"outputs\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=1000,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    # Reduce the batch size for evaluation\n",
        "    per_device_eval_batch_size=1,\n",
        "    # Accumulate gradients during evaluation\n",
        "    eval_accumulation_steps=2,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Inicializaci√≥n del entrenador con el modelo, tokenizer, dataset y argumentos de entrenamiento\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    dataset_num_proc=2,\n",
        "    callbacks=[LoggingCallback]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2C2zF9Uwj_1",
        "outputId": "324645bc-3d07-4c91-b886-4e3824d4b526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar el proceso de entrenamiento\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Guardar el modelo afinado\n",
        "new_model = \"alecrosales1/Llama3_rac\"\n",
        "trainer.model.save_pretrained(new_model)\n",
        "\n",
        "# Cargar el modelo base\n",
        "model_id = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# Fusionar el modelo con los pesos LoRA\n",
        "merged_model = base_model.from_pretrained(new_model)\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "# Guardar el modelo fusionado\n",
        "merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"merged_model\")\n",
        "\n",
        "# Mostrar las estad√≠sticas de entrenamiento\n",
        "trainer_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "syauSE4jwmY0",
        "outputId": "9e174608-7283-4c31-f382-a515bc396754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 2:26:53, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.499700</td>\n",
              "      <td>0.539858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.507600</td>\n",
              "      <td>0.508099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.505400</td>\n",
              "      <td>0.488490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.483700</td>\n",
              "      <td>0.474602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.481600</td>\n",
              "      <td>0.460746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.447900</td>\n",
              "      <td>0.449369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.421900</td>\n",
              "      <td>0.440194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.424900</td>\n",
              "      <td>0.431984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.451200</td>\n",
              "      <td>0.425376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.417500</td>\n",
              "      <td>0.422251</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10: {'loss': 1.6314, 'grad_norm': 2.1105234622955322, 'learning_rate': 0.00019899497487437187, 'epoch': 0.003631411711302769, 'step': 10}\n",
            "Step 20: {'loss': 0.7188, 'grad_norm': 0.4010428786277771, 'learning_rate': 0.0001969849246231156, 'epoch': 0.007262823422605538, 'step': 20}\n",
            "Step 30: {'loss': 0.6363, 'grad_norm': 0.433738648891449, 'learning_rate': 0.0001949748743718593, 'epoch': 0.010894235133908307, 'step': 30}\n",
            "Step 40: {'loss': 0.5889, 'grad_norm': 0.3442542850971222, 'learning_rate': 0.000192964824120603, 'epoch': 0.014525646845211076, 'step': 40}\n",
            "Step 50: {'loss': 0.5418, 'grad_norm': 0.3842291533946991, 'learning_rate': 0.00019095477386934674, 'epoch': 0.018157058556513846, 'step': 50}\n",
            "Step 60: {'loss': 0.5564, 'grad_norm': 0.3663150370121002, 'learning_rate': 0.00018894472361809047, 'epoch': 0.021788470267816613, 'step': 60}\n",
            "Step 70: {'loss': 0.5923, 'grad_norm': 0.361740380525589, 'learning_rate': 0.0001869346733668342, 'epoch': 0.02541988197911938, 'step': 70}\n",
            "Step 80: {'loss': 0.5292, 'grad_norm': 0.29347720742225647, 'learning_rate': 0.0001849246231155779, 'epoch': 0.029051293690422152, 'step': 80}\n",
            "Step 90: {'loss': 0.5406, 'grad_norm': 0.33600401878356934, 'learning_rate': 0.0001829145728643216, 'epoch': 0.032682705401724924, 'step': 90}\n",
            "Step 100: {'loss': 0.4997, 'grad_norm': 0.31716349720954895, 'learning_rate': 0.00018090452261306533, 'epoch': 0.03631411711302769, 'step': 100}\n",
            "Step 100: {'eval_loss': 0.539858341217041, 'eval_runtime': 512.634, 'eval_samples_per_second': 4.775, 'eval_steps_per_second': 4.775, 'epoch': 0.03631411711302769, 'step': 100}\n",
            "Step 110: {'loss': 0.6166, 'grad_norm': 0.2973945438861847, 'learning_rate': 0.00017889447236180906, 'epoch': 0.03994552882433046, 'step': 110}\n",
            "Step 120: {'loss': 0.5582, 'grad_norm': 0.3343980312347412, 'learning_rate': 0.0001768844221105528, 'epoch': 0.04357694053563323, 'step': 120}\n",
            "Step 130: {'loss': 0.5279, 'grad_norm': 0.35084444284439087, 'learning_rate': 0.0001748743718592965, 'epoch': 0.047208352246935995, 'step': 130}\n",
            "Step 140: {'loss': 0.5673, 'grad_norm': 0.28885817527770996, 'learning_rate': 0.0001728643216080402, 'epoch': 0.05083976395823876, 'step': 140}\n",
            "Step 150: {'loss': 0.5267, 'grad_norm': 0.42615315318107605, 'learning_rate': 0.00017085427135678393, 'epoch': 0.05447117566954154, 'step': 150}\n",
            "Step 160: {'loss': 0.541, 'grad_norm': 0.37267106771469116, 'learning_rate': 0.00016884422110552766, 'epoch': 0.058102587380844305, 'step': 160}\n",
            "Step 170: {'loss': 0.5103, 'grad_norm': 0.38629719614982605, 'learning_rate': 0.00016683417085427136, 'epoch': 0.06173399909214707, 'step': 170}\n",
            "Step 180: {'loss': 0.5458, 'grad_norm': 0.3264678716659546, 'learning_rate': 0.0001648241206030151, 'epoch': 0.06536541080344985, 'step': 180}\n",
            "Step 190: {'loss': 0.5286, 'grad_norm': 0.4723131060600281, 'learning_rate': 0.0001628140703517588, 'epoch': 0.06899682251475261, 'step': 190}\n",
            "Step 200: {'loss': 0.5076, 'grad_norm': 0.41974952816963196, 'learning_rate': 0.00016080402010050252, 'epoch': 0.07262823422605538, 'step': 200}\n",
            "Step 200: {'eval_loss': 0.5080991387367249, 'eval_runtime': 512.273, 'eval_samples_per_second': 4.779, 'eval_steps_per_second': 4.779, 'epoch': 0.07262823422605538, 'step': 200}\n",
            "Step 210: {'loss': 0.5182, 'grad_norm': 0.3691420257091522, 'learning_rate': 0.00015879396984924625, 'epoch': 0.07625964593735815, 'step': 210}\n",
            "Step 220: {'loss': 0.5127, 'grad_norm': 0.2991994023323059, 'learning_rate': 0.00015678391959798995, 'epoch': 0.07989105764866092, 'step': 220}\n",
            "Step 230: {'loss': 0.5146, 'grad_norm': 0.41913822293281555, 'learning_rate': 0.00015477386934673368, 'epoch': 0.08352246935996369, 'step': 230}\n",
            "Step 240: {'loss': 0.5299, 'grad_norm': 0.3425392806529999, 'learning_rate': 0.00015276381909547739, 'epoch': 0.08715388107126645, 'step': 240}\n",
            "Step 250: {'loss': 0.5149, 'grad_norm': 0.3130912184715271, 'learning_rate': 0.00015075376884422112, 'epoch': 0.09078529278256922, 'step': 250}\n",
            "Step 260: {'loss': 0.5348, 'grad_norm': 0.3589341938495636, 'learning_rate': 0.00014874371859296482, 'epoch': 0.09441670449387199, 'step': 260}\n",
            "Step 270: {'loss': 0.4693, 'grad_norm': 0.33074715733528137, 'learning_rate': 0.00014673366834170855, 'epoch': 0.09804811620517476, 'step': 270}\n",
            "Step 280: {'loss': 0.5194, 'grad_norm': 0.34708309173583984, 'learning_rate': 0.00014472361809045228, 'epoch': 0.10167952791647752, 'step': 280}\n",
            "Step 290: {'loss': 0.5615, 'grad_norm': 0.3927859961986542, 'learning_rate': 0.00014271356783919598, 'epoch': 0.1053109396277803, 'step': 290}\n",
            "Step 300: {'loss': 0.5054, 'grad_norm': 0.3660423159599304, 'learning_rate': 0.0001407035175879397, 'epoch': 0.10894235133908307, 'step': 300}\n",
            "Step 300: {'eval_loss': 0.48848989605903625, 'eval_runtime': 512.4602, 'eval_samples_per_second': 4.777, 'eval_steps_per_second': 4.777, 'epoch': 0.10894235133908307, 'step': 300}\n",
            "Step 310: {'loss': 0.5234, 'grad_norm': 0.39268726110458374, 'learning_rate': 0.0001386934673366834, 'epoch': 0.11257376305038584, 'step': 310}\n",
            "Step 320: {'loss': 0.4926, 'grad_norm': 0.38415101170539856, 'learning_rate': 0.00013668341708542714, 'epoch': 0.11620517476168861, 'step': 320}\n",
            "Step 330: {'loss': 0.4971, 'grad_norm': 0.37169149518013, 'learning_rate': 0.00013467336683417087, 'epoch': 0.11983658647299138, 'step': 330}\n",
            "Step 340: {'loss': 0.5057, 'grad_norm': 0.34774962067604065, 'learning_rate': 0.00013266331658291457, 'epoch': 0.12346799818429414, 'step': 340}\n",
            "Step 350: {'loss': 0.4845, 'grad_norm': 0.38801252841949463, 'learning_rate': 0.0001306532663316583, 'epoch': 0.12709940989559693, 'step': 350}\n",
            "Step 360: {'loss': 0.4928, 'grad_norm': 0.3304005563259125, 'learning_rate': 0.000128643216080402, 'epoch': 0.1307308216068997, 'step': 360}\n",
            "Step 370: {'loss': 0.513, 'grad_norm': 0.3888086974620819, 'learning_rate': 0.00012663316582914574, 'epoch': 0.13436223331820246, 'step': 370}\n",
            "Step 380: {'loss': 0.4913, 'grad_norm': 0.3443528711795807, 'learning_rate': 0.00012462311557788947, 'epoch': 0.13799364502950523, 'step': 380}\n",
            "Step 390: {'loss': 0.4814, 'grad_norm': 0.34484046697616577, 'learning_rate': 0.00012261306532663317, 'epoch': 0.141625056740808, 'step': 390}\n",
            "Step 400: {'loss': 0.4837, 'grad_norm': 0.36249682307243347, 'learning_rate': 0.00012060301507537688, 'epoch': 0.14525646845211077, 'step': 400}\n",
            "Step 400: {'eval_loss': 0.47460174560546875, 'eval_runtime': 512.6168, 'eval_samples_per_second': 4.775, 'eval_steps_per_second': 4.775, 'epoch': 0.14525646845211077, 'step': 400}\n",
            "Step 410: {'loss': 0.4664, 'grad_norm': 0.3823721408843994, 'learning_rate': 0.00011859296482412061, 'epoch': 0.14888788016341353, 'step': 410}\n",
            "Step 420: {'loss': 0.5111, 'grad_norm': 0.38090434670448303, 'learning_rate': 0.00011658291457286432, 'epoch': 0.1525192918747163, 'step': 420}\n",
            "Step 430: {'loss': 0.4543, 'grad_norm': 0.35904988646507263, 'learning_rate': 0.00011457286432160806, 'epoch': 0.15615070358601907, 'step': 430}\n",
            "Step 440: {'loss': 0.4585, 'grad_norm': 0.3774546980857849, 'learning_rate': 0.00011256281407035176, 'epoch': 0.15978211529732184, 'step': 440}\n",
            "Step 450: {'loss': 0.4983, 'grad_norm': 0.37420451641082764, 'learning_rate': 0.00011055276381909548, 'epoch': 0.1634135270086246, 'step': 450}\n",
            "Step 460: {'loss': 0.4803, 'grad_norm': 0.38443323969841003, 'learning_rate': 0.00010854271356783921, 'epoch': 0.16704493871992737, 'step': 460}\n",
            "Step 470: {'loss': 0.4752, 'grad_norm': 0.37705570459365845, 'learning_rate': 0.00010653266331658291, 'epoch': 0.17067635043123014, 'step': 470}\n",
            "Step 480: {'loss': 0.4837, 'grad_norm': 0.3917020559310913, 'learning_rate': 0.00010452261306532664, 'epoch': 0.1743077621425329, 'step': 480}\n",
            "Step 490: {'loss': 0.4383, 'grad_norm': 0.3861951529979706, 'learning_rate': 0.00010251256281407036, 'epoch': 0.17793917385383567, 'step': 490}\n",
            "Step 500: {'loss': 0.4816, 'grad_norm': 0.3816602826118469, 'learning_rate': 0.00010050251256281407, 'epoch': 0.18157058556513844, 'step': 500}\n",
            "Step 500: {'eval_loss': 0.460746169090271, 'eval_runtime': 512.2709, 'eval_samples_per_second': 4.779, 'eval_steps_per_second': 4.779, 'epoch': 0.18157058556513844, 'step': 500}\n",
            "Step 510: {'loss': 0.4917, 'grad_norm': 0.3561587929725647, 'learning_rate': 9.84924623115578e-05, 'epoch': 0.1852019972764412, 'step': 510}\n",
            "Step 520: {'loss': 0.479, 'grad_norm': 0.40728509426116943, 'learning_rate': 9.64824120603015e-05, 'epoch': 0.18883340898774398, 'step': 520}\n",
            "Step 530: {'loss': 0.4729, 'grad_norm': 0.43638044595718384, 'learning_rate': 9.447236180904523e-05, 'epoch': 0.19246482069904675, 'step': 530}\n",
            "Step 540: {'loss': 0.465, 'grad_norm': 0.417783260345459, 'learning_rate': 9.246231155778895e-05, 'epoch': 0.1960962324103495, 'step': 540}\n",
            "Step 550: {'loss': 0.451, 'grad_norm': 0.34973201155662537, 'learning_rate': 9.045226130653267e-05, 'epoch': 0.19972764412165228, 'step': 550}\n",
            "Step 560: {'loss': 0.4872, 'grad_norm': 0.41211825609207153, 'learning_rate': 8.84422110552764e-05, 'epoch': 0.20335905583295505, 'step': 560}\n",
            "Step 570: {'loss': 0.4622, 'grad_norm': 0.3916178345680237, 'learning_rate': 8.64321608040201e-05, 'epoch': 0.20699046754425782, 'step': 570}\n",
            "Step 580: {'loss': 0.4446, 'grad_norm': 0.3731808364391327, 'learning_rate': 8.442211055276383e-05, 'epoch': 0.2106218792555606, 'step': 580}\n",
            "Step 590: {'loss': 0.4404, 'grad_norm': 0.3842357397079468, 'learning_rate': 8.241206030150754e-05, 'epoch': 0.21425329096686338, 'step': 590}\n",
            "Step 600: {'loss': 0.4479, 'grad_norm': 0.39145946502685547, 'learning_rate': 8.040201005025126e-05, 'epoch': 0.21788470267816615, 'step': 600}\n",
            "Step 600: {'eval_loss': 0.449368953704834, 'eval_runtime': 512.6736, 'eval_samples_per_second': 4.775, 'eval_steps_per_second': 4.775, 'epoch': 0.21788470267816615, 'step': 600}\n",
            "Step 610: {'loss': 0.4489, 'grad_norm': 0.38111528754234314, 'learning_rate': 7.839195979899498e-05, 'epoch': 0.22151611438946892, 'step': 610}\n",
            "Step 620: {'loss': 0.4598, 'grad_norm': 0.40967726707458496, 'learning_rate': 7.638190954773869e-05, 'epoch': 0.22514752610077168, 'step': 620}\n",
            "Step 630: {'loss': 0.4367, 'grad_norm': 0.4231009781360626, 'learning_rate': 7.437185929648241e-05, 'epoch': 0.22877893781207445, 'step': 630}\n",
            "Step 640: {'loss': 0.4917, 'grad_norm': 0.4157176613807678, 'learning_rate': 7.236180904522614e-05, 'epoch': 0.23241034952337722, 'step': 640}\n",
            "Step 650: {'loss': 0.4486, 'grad_norm': 0.45087283849716187, 'learning_rate': 7.035175879396985e-05, 'epoch': 0.23604176123468, 'step': 650}\n",
            "Step 660: {'loss': 0.4283, 'grad_norm': 0.40138915181159973, 'learning_rate': 6.834170854271357e-05, 'epoch': 0.23967317294598275, 'step': 660}\n",
            "Step 670: {'loss': 0.4243, 'grad_norm': 0.41590139269828796, 'learning_rate': 6.633165829145729e-05, 'epoch': 0.24330458465728552, 'step': 670}\n",
            "Step 680: {'loss': 0.4401, 'grad_norm': 0.389352411031723, 'learning_rate': 6.4321608040201e-05, 'epoch': 0.2469359963685883, 'step': 680}\n",
            "Step 690: {'loss': 0.4379, 'grad_norm': 0.3954280912876129, 'learning_rate': 6.231155778894473e-05, 'epoch': 0.2505674080798911, 'step': 690}\n",
            "Step 700: {'loss': 0.4219, 'grad_norm': 0.3378486931324005, 'learning_rate': 6.030150753768844e-05, 'epoch': 0.25419881979119385, 'step': 700}\n",
            "Step 700: {'eval_loss': 0.44019392132759094, 'eval_runtime': 512.2646, 'eval_samples_per_second': 4.779, 'eval_steps_per_second': 4.779, 'epoch': 0.25419881979119385, 'step': 700}\n",
            "Step 710: {'loss': 0.4486, 'grad_norm': 0.36622291803359985, 'learning_rate': 5.829145728643216e-05, 'epoch': 0.2578302315024966, 'step': 710}\n",
            "Step 720: {'loss': 0.4606, 'grad_norm': 0.37884002923965454, 'learning_rate': 5.628140703517588e-05, 'epoch': 0.2614616432137994, 'step': 720}\n",
            "Step 730: {'loss': 0.432, 'grad_norm': 0.4037039577960968, 'learning_rate': 5.4271356783919604e-05, 'epoch': 0.26509305492510216, 'step': 730}\n",
            "Step 740: {'loss': 0.4784, 'grad_norm': 0.42649781703948975, 'learning_rate': 5.226130653266332e-05, 'epoch': 0.2687244666364049, 'step': 740}\n",
            "Step 750: {'loss': 0.4245, 'grad_norm': 0.41166573762893677, 'learning_rate': 5.0251256281407036e-05, 'epoch': 0.2723558783477077, 'step': 750}\n",
            "Step 760: {'loss': 0.4725, 'grad_norm': 0.3466498851776123, 'learning_rate': 4.824120603015075e-05, 'epoch': 0.27598729005901046, 'step': 760}\n",
            "Step 770: {'loss': 0.4712, 'grad_norm': 0.41090407967567444, 'learning_rate': 4.6231155778894475e-05, 'epoch': 0.2796187017703132, 'step': 770}\n",
            "Step 780: {'loss': 0.4471, 'grad_norm': 0.4096313416957855, 'learning_rate': 4.42211055276382e-05, 'epoch': 0.283250113481616, 'step': 780}\n",
            "Step 790: {'loss': 0.4296, 'grad_norm': 0.40992531180381775, 'learning_rate': 4.2211055276381914e-05, 'epoch': 0.28688152519291876, 'step': 790}\n",
            "Step 800: {'loss': 0.4249, 'grad_norm': 0.41736990213394165, 'learning_rate': 4.020100502512563e-05, 'epoch': 0.29051293690422153, 'step': 800}\n",
            "Step 800: {'eval_loss': 0.4319842457771301, 'eval_runtime': 512.3642, 'eval_samples_per_second': 4.778, 'eval_steps_per_second': 4.778, 'epoch': 0.29051293690422153, 'step': 800}\n",
            "Step 810: {'loss': 0.4241, 'grad_norm': 0.4384515583515167, 'learning_rate': 3.8190954773869346e-05, 'epoch': 0.2941443486155243, 'step': 810}\n",
            "Step 820: {'loss': 0.4496, 'grad_norm': 0.4154162108898163, 'learning_rate': 3.618090452261307e-05, 'epoch': 0.29777576032682707, 'step': 820}\n",
            "Step 830: {'loss': 0.4664, 'grad_norm': 0.3719322979450226, 'learning_rate': 3.4170854271356785e-05, 'epoch': 0.30140717203812983, 'step': 830}\n",
            "Step 840: {'loss': 0.4565, 'grad_norm': 0.3597119450569153, 'learning_rate': 3.21608040201005e-05, 'epoch': 0.3050385837494326, 'step': 840}\n",
            "Step 850: {'loss': 0.4493, 'grad_norm': 0.4751853346824646, 'learning_rate': 3.015075376884422e-05, 'epoch': 0.30866999546073537, 'step': 850}\n",
            "Step 860: {'loss': 0.4603, 'grad_norm': 0.4546041488647461, 'learning_rate': 2.814070351758794e-05, 'epoch': 0.31230140717203814, 'step': 860}\n",
            "Step 870: {'loss': 0.4097, 'grad_norm': 0.35585254430770874, 'learning_rate': 2.613065326633166e-05, 'epoch': 0.3159328188833409, 'step': 870}\n",
            "Step 880: {'loss': 0.4499, 'grad_norm': 0.38983872532844543, 'learning_rate': 2.4120603015075376e-05, 'epoch': 0.3195642305946437, 'step': 880}\n",
            "Step 890: {'loss': 0.4586, 'grad_norm': 0.44825753569602966, 'learning_rate': 2.21105527638191e-05, 'epoch': 0.32319564230594644, 'step': 890}\n",
            "Step 900: {'loss': 0.4512, 'grad_norm': 0.4072413742542267, 'learning_rate': 2.0100502512562815e-05, 'epoch': 0.3268270540172492, 'step': 900}\n",
            "Step 900: {'eval_loss': 0.4253762364387512, 'eval_runtime': 512.432, 'eval_samples_per_second': 4.777, 'eval_steps_per_second': 4.777, 'epoch': 0.3268270540172492, 'step': 900}\n",
            "Step 910: {'loss': 0.4272, 'grad_norm': 0.4347361624240875, 'learning_rate': 1.8090452261306535e-05, 'epoch': 0.330458465728552, 'step': 910}\n",
            "Step 920: {'loss': 0.4391, 'grad_norm': 0.41321370005607605, 'learning_rate': 1.608040201005025e-05, 'epoch': 0.33408987743985474, 'step': 920}\n",
            "Step 930: {'loss': 0.4665, 'grad_norm': 0.3590618371963501, 'learning_rate': 1.407035175879397e-05, 'epoch': 0.3377212891511575, 'step': 930}\n",
            "Step 940: {'loss': 0.4197, 'grad_norm': 0.42472296953201294, 'learning_rate': 1.2060301507537688e-05, 'epoch': 0.3413527008624603, 'step': 940}\n",
            "Step 950: {'loss': 0.4082, 'grad_norm': 0.3846476376056671, 'learning_rate': 1.0050251256281408e-05, 'epoch': 0.34498411257376305, 'step': 950}\n",
            "Step 960: {'loss': 0.4236, 'grad_norm': 0.3730470836162567, 'learning_rate': 8.040201005025125e-06, 'epoch': 0.3486155242850658, 'step': 960}\n",
            "Step 970: {'loss': 0.4196, 'grad_norm': 0.42060887813568115, 'learning_rate': 6.030150753768844e-06, 'epoch': 0.3522469359963686, 'step': 970}\n",
            "Step 980: {'loss': 0.4212, 'grad_norm': 0.41711774468421936, 'learning_rate': 4.020100502512563e-06, 'epoch': 0.35587834770767135, 'step': 980}\n",
            "Step 990: {'loss': 0.4497, 'grad_norm': 0.43063193559646606, 'learning_rate': 2.0100502512562813e-06, 'epoch': 0.3595097594189741, 'step': 990}\n",
            "Step 1000: {'loss': 0.4175, 'grad_norm': 0.4575830101966858, 'learning_rate': 0.0, 'epoch': 0.3631411711302769, 'step': 1000}\n",
            "Step 1000: {'eval_loss': 0.42225074768066406, 'eval_runtime': 512.512, 'eval_samples_per_second': 4.776, 'eval_steps_per_second': 4.776, 'epoch': 0.3631411711302769, 'step': 1000}\n",
            "Step 1000: {'train_runtime': 8816.8712, 'train_samples_per_second': 0.907, 'train_steps_per_second': 0.113, 'total_flos': 8.775198919552205e+16, 'train_loss': 0.4949662251472473, 'epoch': 0.3631411711302769, 'step': 1000}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c3baac4786ed>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Cargar el modelo base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unsloth/llama-3-8b-bnb-4bit\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3752\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3753\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3754\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3755\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3756\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4212\u001b[0m                                 )\n\u001b[1;32m   4213\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4214\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4215\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4216\u001b[0m                             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_quantized_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;31m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;31m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mcreate_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    199\u001b[0m                         \u001b[0munexpected_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             new_value = bnb.nn.Params4bit.from_prequantized(\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mquantized_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantized_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mfrom_prequantized\u001b[0;34m(cls, data, quantized_stats, requires_grad, device, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     ) -> \"Params4bit\":\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_subclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuantState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantized_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.push_to_hub(\"alecrosales1/GemmaRac_Alec\", use_temp_dir=True)\n",
        "tokenizer.push_to_hub(\"alecrosales1/GemmaRac_Alec\", use_temp_dir=True)\n"
      ],
      "metadata": {
        "id": "75teQexgYL2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}